HDFS Docker Management
----------------------

For testing, a HDFS filesystem is supplied using a docker.
This runs an HDFS internally, and exposes two interfaces:
1) The HDFS master on 8020, and
2) The HDFS server2 service on 50070.

Spark accesses HDFS using the HDFS master.
XFrames uses spark through the HDFS server2 service.
Both are supported by the docker.

Scripts
-------
The management scripts are:

$ docker-setup
Runs the HDFS docker and initializes an empty HDFS.
The hdfs filesystem is mapped to /tmp/cache, so that it is persistent
across different runs of the docker.
$XPATTERNS_HOME/dockdir is also exposed within the docker via /opt/xpatterns.

$ docker-run
Runs the HDFS docker without reinitializing hdfs.

$ docker-stop
Stops and removes the HDFS docker.

$ docker-ps
Lists the running dockers.

$ docker-connect
Connects to an interactive shell within the HDFS docker.

$ docker-init-test-data
Initializes the test data in the HDFS docker.
You need to wait a minute after starting the docker to let it get fully started.


Hosts
-----
The docker returns URLs with its hostname (xpatterns-hadoop) embedded.
To resolve these properly, add the following line to /etc/hosts:
127.0.0.1 xpatterns-hadoop
If you do not do this, several of the test_save tests will fail.

Unit Tests
----------
Tests of HDFS capabilities are in xframes/test/testhdfs.py.
These depend on configuration settings in default.ini.
These tests assume the presence of preexisting files, which
are created using the script in dockdir/scripts/hdfs-setup.
Before running testhdfs:
1) Run docker-setup.
2) Make sure /etc/hosts has been updated, as described above.
3) Run docker-init-test-data to copy files to hdfs.
